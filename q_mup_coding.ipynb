{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scZxX6oEKzZn"
      },
      "source": [
        "## Maximal Update Parameterization\n",
        "\n",
        "In this problem, we will examine the training of a simple MLP with hidden layers of varying widths. We will then investigate the maximal update parameterization (muP) which will allow us to use a single global learning rate to jointly train layers of any width.\n",
        "\n",
        "Note: This homework question is new this year and it is messier than usual. We felt it was worth it to get it out so you can play with these new techniques. If you're feeling stuck, don't hesistate to ask questions on Ed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRp2k4Dbli_k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import sys, os\n",
        "# Import from the package path to avoid fragile sys.path hacks and to make\n",
        "from src.models import MLP\n",
        "from src.optimizers import SimpleAdam\n",
        "from src.training import train_one_step\n",
        "# Optional: autoreload to pick up edits in the src package while the notebook\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load MNIST data\n",
        "(train_images, train_labels), (valid_images, valid_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "valid_images = valid_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_images = torch.from_numpy(train_images)\n",
        "train_labels = torch.from_numpy(train_labels).long()\n",
        "valid_images = torch.from_numpy(valid_images)\n",
        "valid_labels = torch.from_numpy(valid_labels).long()\n",
        "\n",
        "def rms(x, dim):\n",
        "    return torch.sqrt(torch.mean(x**2, dim=dim))\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIYSY34qX80r"
      },
      "outputs": [],
      "source": [
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "train_one_step(\n",
        "    MLP,                                  # mlp\n",
        "    [8, 16, 64, 64, 64, 256, 256, 1024],  # hiddens\n",
        "    SimpleAdam,                           # optimizer\n",
        "    \"Adam\",                               # label\n",
        "    1e-2,                                 # lr\n",
        "    train_images=train_images,\n",
        "    train_labels=train_labels,\n",
        "    batch_idx=batch_idx,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykmT0CjoLaCH"
      },
      "source": [
        "## a. Examining the norms of a heterogenous MLP.\n",
        "\n",
        "Run the above cell, which trains a neural network for a single gradient step, then examines the effect of that step on the resulting activations. What are the dimensions of each layer in the neural network?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "How does the dimensionality of the layer affect the RMS norm of the activation deltas?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "Change the widths of some of your neural network layers, and recreate the plot -- did the RMS values change as expected?\n",
        "\n",
        "*Answer:*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0-GhF6eNN5I"
      },
      "outputs": [],
      "source": [
        "# TODO: Call some plotting code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzosCNjMlMZ"
      },
      "source": [
        "## b. Examining the norms of the updates to the weights.\n",
        "\n",
        "In the provided code above, we plotted the change in norms of the *activation vectors*. Now, you will examine the change in the weights themselves. Create a version of the above function that runs a single gradient step, then for each dense layer plot:\n",
        "- The *Frobenius* norm of the update.\n",
        "- The *spectral* norm of the update.\n",
        "- The *RMS-RMS induced norm* of the update.\n",
        "\n",
        "Which one of these norms correlates the most with the RMS norms of the activations?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "You should calculate your updates as `new_dense_parameter - old_dense_parameter`.\n",
        "\n",
        "TODO: Implement the norm computations in `src/training.py` under the `training_one_step_matrices` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOp2ISs2NfA9"
      },
      "outputs": [],
      "source": [
        "from src.training import train_one_step_matrices\n",
        "\n",
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "train_one_step_matrices(\n",
        "    mlp=MLP,\n",
        "    hiddens=[8, 16, 64, 64, 64, 256, 256, 1024],\n",
        "    optimizer=SimpleAdam,\n",
        "    lr=1e-2,\n",
        "    train_images=train_images,\n",
        "    train_labels=train_labels,\n",
        "    batch_idx=batch_idx,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYleV00ERYKW"
      },
      "source": [
        "## c. Implementing muP\n",
        "\n",
        "We will now implement muP scaling. Modify the starter code below to set a per-layer learning rate such that the resulting RMS activation-deltas are uniform scale, regardless of the layer widths. Plot the resulting activation-deltas on at least two sets of widths.\n",
        "\n",
        "Note: Even with the correct scaling, the first 2-3 activation-deltas may have a lower norm than the rest. Can you think of a reason why this might be the case?\n",
        "\n",
        "TODO: Implement the MuP scaling with Adam optimizers in `src/optimizers.py` file under the `SimpleAdamMuP` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.optimizers import SimpleAdamMuP\n",
        "\n",
        "# First configuration\n",
        "train_one_step(\n",
        "    mlp=MLP,\n",
        "    hiddens=[8, 16, 64, 64, 64, 256, 256, 1024],     # layer widths\n",
        "    optimizer=SimpleAdamMuP,                         # optimizer class\n",
        "    label=\"Adam MuP\",                                # label for the plot\n",
        "    lr=2,                                            # learning rate\n",
        "    train_images=train_images,                       # training data\n",
        "    train_labels=train_labels,\n",
        "    device=get_device()                                   # device (CPU/GPU)\n",
        ")\n",
        "\n",
        "# Second configuration\n",
        "train_one_step(\n",
        "    mlp=MLP,\n",
        "    hiddens=[8, 16, 32, 64, 128, 256, 512, 1024],\n",
        "    optimizer=SimpleAdamMuP,\n",
        "    label=\"Adam MuP\",\n",
        "    lr=2,\n",
        "    train_images=train_images,\n",
        "    train_labels=train_labels,\n",
        "    device=get_device()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GznnkrDAjV1e"
      },
      "source": [
        "## d. Per-Weight Multipliers\n",
        "\n",
        "An alternative way to implement muP is to adjust the *network graph* itself, rather than the optimizer. Implement this below, and recreate the above uniformly-scaled graph when using the *Adam* (not muP) optimizer. We have disabled biases to simplify the problem.\n",
        "\n",
        "Why is multiplying the output of a layer by a constant the same as adjusting the learning-rate of that layer (when using Adam or SignGD)?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "Note that scaling down the activation in this manner effectively scales down our weight initializations by a factor of $d_{in}$, so we must scale up our weight initializations to compensate and recover the correct muP initialization.\n",
        "\n",
        "TODO: Implement the MuP scaling within the MLP in `src/models.py` under the `ScaledMLP` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XueS5-OjoMh"
      },
      "outputs": [],
      "source": [
        "from src.models import ScaledMLP\n",
        "\n",
        "train_one_step(\n",
        "    mlp=ScaledMLP,\n",
        "    hiddens=[8, 16, 64, 64, 64, 256, 256, 1024],\n",
        "    optimizer=SimpleAdam,\n",
        "    label=\"Adam\",\n",
        "    lr=1e-2,\n",
        "    train_images=train_images,\n",
        "    train_labels=train_labels,\n",
        "    batch_idx=batch_idx,\n",
        "    device=get_device()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHHRVpQMiEvS"
      },
      "source": [
        "## e. Hyperparameter Transfer\n",
        "\n",
        "Run the following code, which will perform a sweep over learning rates for 3-layer MLPs of increasing width using Adam. How does the optimal learning rate change as the network increases in size?\n",
        "\n",
        "*Answer:*\n",
        "In the second cell, we will instead use the muP optimizer you implemented. How does the optimal learning rate work now? You should aim to show that there is a single global learning rate that works on a majority of widths. The 256-width network should achieve a loss of 0.5, comparable to Adam.\n",
        "\n",
        "*Answer:*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_z3UhD8Ytts"
      },
      "outputs": [],
      "source": [
        "valid_idx = np.random.randint(0, len(train_images), size=64)\n",
        "valid_images = train_images[valid_idx]\n",
        "valid_labels = train_labels[valid_idx]\n",
        "valid_images, valid_labels = valid_images.to(device), valid_labels.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEIPwU8SZtIz"
      },
      "outputs": [],
      "source": [
        "from src.training import train_with_lr\n",
        "\n",
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]\n",
        "adam_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        adam_results[wi, lri] = train_with_lr(\n",
        "            mlp=MLP,\n",
        "            hiddens=[width, width, width],\n",
        "            optimizer=SimpleAdam,          # ← required\n",
        "            lr=lr,\n",
        "            train_images=train_images,\n",
        "            train_labels=train_labels,\n",
        "            valid_images=valid_images,\n",
        "            valid_labels=valid_labels,\n",
        "            device=get_device(),\n",
        "            steps=100,\n",
        "        )\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate (Adam)')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), adam_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTHetldFdMOP"
      },
      "outputs": [],
      "source": [
        "from src.training import train_with_lr\n",
        "from src.optimizers import SimpleAdamMuP\n",
        "\n",
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.03, 0.1, 0.3, 1.0, 3.0, 10.0]\n",
        "adamMup_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        adamMup_results[wi, lri] = train_with_lr(\n",
        "            mlp=MLP,\n",
        "            hiddens=[width, width, width],\n",
        "            optimizer=SimpleAdamMuP,         # ← required\n",
        "            lr=lr,\n",
        "            train_images=train_images,\n",
        "            train_labels=train_labels,\n",
        "            valid_images=valid_images,\n",
        "            valid_labels=valid_labels,\n",
        "            device=get_device(),\n",
        "            steps=100,\n",
        "        )\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate (muP)')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), adamMup_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaqzanKomMTx"
      },
      "source": [
        "## e. Shampoo and Orthogonalization\n",
        "\n",
        "In lecture, we discussed a simplified version of the Shampoo update, which can be viewed as *orthogonalizing* the update to a dense layer. In the following code block, implement this simplified Shampoo update:\n",
        "\n",
        "$$\n",
        "momentum \\rightarrow U \\Sigma V^T. \\qquad update = UV^T.\n",
        "$$\n",
        "\n",
        "Feel free to use linear algebra functions such as `torch.linalg.svd`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIoTUgUYmvJt"
      },
      "source": [
        "Now, we will examine the relationship between the Frobenius norm and the Spectral norm for Adam vs. Shampoo. Plot these norms using your code from part c. What relationship do you see? Can you come up for a reason why this makes sense?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "Bonus: How should we scale the Shampoo update so the *induced RMS-RMS norm* is equal? Implement this change.\n",
        "\n",
        "*Answer:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: Implement the Shampoo and Scaled Shampoo optimizers in `src/optimizers.py` under the `SimpleShampoo` and `SimpleShampooScaled` classes respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pcGr3WloaGw"
      },
      "outputs": [],
      "source": [
        "from src.optimizers import SimpleShampoo, SimpleShampooScaled\n",
        "\n",
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "train_one_step_matrices(mlp=MLP, optimizer=SimpleAdam, label=\"Adam\", train_images=train_images, train_labels=train_labels, batch_idx=batch_idx, device=device)\n",
        "train_one_step_matrices(mlp=MLP, optimizer=SimpleShampoo, label=\"Shampoo\", train_images=train_images, train_labels=train_labels, batch_idx=batch_idx, device=device)\n",
        "train_one_step_matrices(mlp=MLP, optimizer=SimpleShampooScaled, label=\"ShampooScaled\", train_images=train_images, train_labels=train_labels, batch_idx=batch_idx, device=device)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "fodl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
