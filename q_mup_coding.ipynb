{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scZxX6oEKzZn"
      },
      "source": [
        "## Maximal Update Parameterization\n",
        "\n",
        "In this problem, we will examine the training of a simple MLP with hidden layers of varying widths. We will then investigate the maximal update parameterization (muP) which will allow us to use a single global learning rate to jointly train layers of any width.\n",
        "\n",
        "Note: This homework question is new this year and it is messier than usual. We felt it was worth it to get it out so you can play with these new techniques. If you're feeling stuck, don't hesistate to ask questions on Ed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRp2k4Dbli_k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anders/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n",
        "        super(MLP, self).__init__()\n",
        "        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(all_hidden_sizes)-1):\n",
        "            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1]))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = []\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.sigmoid(layer(x))\n",
        "            activations.append(x)\n",
        "        x = self.layers[-1](x)\n",
        "        activations = activations[1:]\n",
        "        return x, [a.detach() for a in activations]\n",
        "\n",
        "# Load MNIST data\n",
        "(train_images, train_labels), (valid_images, valid_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "train_images = train_images.astype(np.float32) / 255.0\n",
        "valid_images = valid_images.astype(np.float32) / 255.0\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_images = torch.from_numpy(train_images)\n",
        "train_labels = torch.from_numpy(train_labels).long()\n",
        "valid_images = torch.from_numpy(valid_images)\n",
        "valid_labels = torch.from_numpy(valid_labels).long()\n",
        "\n",
        "def rms(x, dim):\n",
        "    return torch.sqrt(torch.mean(x**2, dim=dim))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j68v19tfX5Vq"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleAdam(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "        b2: float = 0.999,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1, b2=b2,)\n",
        "        super(SimpleAdam, self).__init__(params, defaults)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XIYSY34qX80r"
      },
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m     axs\u001b[38;5;241m.\u001b[39mset_xticklabels(hiddens[\u001b[38;5;241m1\u001b[39m:])\n\u001b[1;32m     33\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSimpleAdam\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mtrain_one_step\u001b[0;34m(mlp, hiddens, optimizer, label, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels_batch)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m([a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m activations])\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:1064\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure: Optional[Callable[[], \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step to update parameter.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m        closure (Callable): A closure that reevaluates the model and\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m            returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "def train_one_step(mlp=MLP, hiddens=[8, 16, 64, 64, 64, 256, 256, 1024], optimizer=SimpleAdam, label=\"Adam\", lr=0.01):\n",
        "    model = mlp(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "\n",
        "    prev_activations = None\n",
        "    for i in range(2):\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i > 0:\n",
        "            print([a.shape for a in activations])\n",
        "            activation_deltas = [a - pa for a, pa in zip(activations, prev_activations)]\n",
        "            activation_deltas_rms = [torch.mean(rms(a, dim=-1)) for a in activation_deltas]\n",
        "        prev_activations = activations\n",
        "\n",
        "    # plot deltas\n",
        "    deltas = np.array(activation_deltas_rms)\n",
        "    fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "    axs.set_title(f'RMS of activation deltas per layer ({label})')\n",
        "    axs.set_xlabel('Hidden Size of activation')\n",
        "    axs.bar(np.arange(deltas.shape[0]), deltas)\n",
        "    axs.set_xticks(np.arange(deltas.shape[0]))\n",
        "    axs.set_xticklabels(hiddens[1:])\n",
        "    plt.show()\n",
        "train_one_step(optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykmT0CjoLaCH"
      },
      "source": [
        "## a. Examining the norms of a heterogenous MLP.\n",
        "\n",
        "Run the above cell, which trains a neural network for a single gradient step, then examines the effect of that step on the resulting activations. What are the dimensions of each layer in the neural network?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "How does the dimensionality of the layer affect the RMS norm of the activation deltas?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "Change the widths of some of your neural network layers, and recreate the plot -- did the RMS values change as expected?\n",
        "\n",
        "*Answer:*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0-GhF6eNN5I"
      },
      "outputs": [],
      "source": [
        "# TODO: Call some plotting code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOzosCNjMlMZ"
      },
      "source": [
        "## b. Examining the norms of the updates to the weights.\n",
        "\n",
        "In the provided code above, we plotted the change in norms of the *activation vectors*. Now, you will examine the change in the weights themselves. Create a version of the above function that runs a single gradient step, then for each dense layer plot:\n",
        "- The *Frobenius* norm of the update.\n",
        "- The *spectral* norm of the update.\n",
        "- The *RMS-RMS induced norm* of the update.\n",
        "\n",
        "Which one of these norms correlates the most with the RMS norms of the activations?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "You should calculate your updates as `new_dense_parameter - old_dense_parameter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lOp2ISs2NfA9"
      },
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m     axs[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xticks(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(induced_norms)), p_shapes, rotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m45\u001b[39m, ha\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 53\u001b[0m \u001b[43mtrain_one_step_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSimpleAdam\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mtrain_one_step_matrices\u001b[0;34m(mlp, hiddens, optimizer, label, lr)\u001b[0m\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels_batch)\n\u001b[1;32m     18\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m new_params \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()]\n\u001b[1;32m     22\u001b[0m delta_params \u001b[38;5;241m=\u001b[39m [new_p \u001b[38;5;241m-\u001b[39m old_p \u001b[38;5;28;01mfor\u001b[39;00m new_p, old_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(new_params, old_params)]\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:1064\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure: Optional[Callable[[], \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step to update parameter.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m        closure (Callable): A closure that reevaluates the model and\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m            returns the loss. Optional for most optimizers.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "### Solution\n",
        "batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "def train_one_step_matrices(mlp=MLP, hiddens=[8, 16, 64, 64, 64, 256, 256, 1024], optimizer=SimpleAdam, label=\"Adam\", lr=0.01):\n",
        "    model = mlp(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "\n",
        "    old_params = [p.detach().clone() for p in model.parameters()]\n",
        "\n",
        "    for i in range(1):\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, activations = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    new_params = [p.detach().clone() for p in model.parameters()]\n",
        "    delta_params = [new_p - old_p for new_p, old_p in zip(new_params, old_params)]\n",
        "\n",
        "    frob_norms = []\n",
        "    spectral_norms = []\n",
        "    induced_norms = []\n",
        "    p_shapes = []\n",
        "\n",
        "    for p in delta_params:\n",
        "        if len(p.shape) == 2:\n",
        "            ### TODO: Log the respective norms.\n",
        "            pass\n",
        "            ###\n",
        "\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    axs[0].set_title(f'Frobenius norm of update per layer ({label})')\n",
        "    axs[0].set_xlabel('Hidden Size of layer')\n",
        "    axs[0].bar(np.arange(len(frob_norms)), frob_norms)\n",
        "    axs[0].set_xticks(np.arange(len(frob_norms)), p_shapes, rotation=45, ha='right')\n",
        "\n",
        "    axs[1].set_title(f'Spectral norm of update per layer ({label})')\n",
        "    axs[1].set_xlabel('Hidden Size of layer')\n",
        "    axs[1].bar(np.arange(len(spectral_norms)), spectral_norms)\n",
        "    axs[1].set_xticks(np.arange(len(spectral_norms)), p_shapes, rotation=45, ha='right')\n",
        "\n",
        "    axs[2].set_title(f'Induced norm of update per layer ({label})')\n",
        "    axs[2].set_xlabel('Hidden Size of layer')\n",
        "    axs[2].bar(np.arange(len(induced_norms)), induced_norms)\n",
        "    axs[2].set_xticks(np.arange(len(induced_norms)), p_shapes, rotation=45, ha='right')\n",
        "    plt.show()\n",
        "train_one_step_matrices(optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYleV00ERYKW"
      },
      "source": [
        "## c. Implementing muP\n",
        "\n",
        "We will now implement muP scaling. Modify the starter code below to set a per-layer learning rate such that the resulting RMS activation-deltas are uniform scale, regardless of the layer widths. Plot the resulting activation-deltas on at least two sets of widths.\n",
        "\n",
        "Note: Even with the correct scaling, the first 2-3 activation-deltas may have a lower norm than the rest. Can you think of a reason why this might be the case?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NotImplementedError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSimpleAdamMuP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdam MuP\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhiddens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_one_step(optimizer\u001b[38;5;241m=\u001b[39mSimpleAdamMuP, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdam MuP\u001b[39m\u001b[38;5;124m\"\u001b[39m, hiddens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m])\n",
            "Cell \u001b[0;32mIn[3], line 17\u001b[0m, in \u001b[0;36mtrain_one_step\u001b[0;34m(mlp, hiddens, optimizer, label, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels_batch)\n\u001b[1;32m     16\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m([a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m activations])\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/.venv/lib/python3.9/site-packages/torch/optim/optimizer.py:516\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m             )\n\u001b[0;32m--> 516\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/.venv/lib/python3.9/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/Berkeley/2025Fall/Designing, Visualizing and Understanding Deep Neural Networks/spC/SpecialParticipationC/q_mup_coding_refactor/src/optimizers.py:124\u001b[0m, in \u001b[0;36mSimpleAdamMuP.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m u \u001b[38;5;241m=\u001b[39m m_hat \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39msqrt(v_hat) \u001b[38;5;241m+\u001b[39m eps)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m### Todo: Adjust the per-layer learning rate scaling factor so per-layer RMS activation deltas are constant.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m### Hint for part e: The following tricks will help you retain performance when using muP scaling.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m###  - For the output layer, we find it is best to ignore the muP scaling, and instead use a fixed learning rate (e.g. 0.003).\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[1;32m    129\u001b[0m p\u001b[38;5;241m.\u001b[39madd_(u, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr)\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "sys.path.append(os.path.abspath(\"q_mup_coding_refactor/src\"))\n",
        "from optimizers import SimpleAdamMuP\n",
        "\n",
        "train_one_step(optimizer=SimpleAdamMuP, lr=2, label=\"Adam MuP\", hiddens=[8, 16, 64, 64, 64, 256, 256, 1024])\n",
        "train_one_step(optimizer=SimpleAdamMuP, lr=2, label=\"Adam MuP\", hiddens=[8, 16, 32, 64, 128, 256, 512, 1024])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GznnkrDAjV1e"
      },
      "source": [
        "## d. Per-Weight Multipliers\n",
        "\n",
        "An alternative way to implement muP is to adjust the *network graph* itself, rather than the optimizer. Implement this below, and recreate the above uniformly-scaled graph when using the *Adam* (not muP) optimizer. We have disabled biases to simplify the problem.\n",
        "\n",
        "Why is multiplying the output of a layer by a constant the same as adjusting the learning-rate of that layer (when using Adam or SignGD)?\n",
        "\n",
        "*Answer:*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XueS5-OjoMh"
      },
      "outputs": [],
      "source": [
        "class ScaledMLP(nn.Module):\n",
        "    def __init__(self, input_size=784, hidden_sizes = [8, 16, 32, 64, 128], num_classes=10):\n",
        "        super(ScaledMLP, self).__init__()\n",
        "        all_hidden_sizes = [input_size] + hidden_sizes + [num_classes]\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(all_hidden_sizes)-1):\n",
        "            self.layers.append(nn.Linear(all_hidden_sizes[i], all_hidden_sizes[i+1], bias=False))\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        activations = []\n",
        "        x = x.view(x.size(0), -1)  # Flatten: (batch_size, 28*28)\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = layer(x)\n",
        "            ## TODO\n",
        "            pass\n",
        "            ##\n",
        "            x = self.sigmoid(x)\n",
        "            activations.append(x)\n",
        "        x = self.layers[-1](x)\n",
        "        activations = activations[1:]\n",
        "        return x, [a.detach() for a in activations]\n",
        "\n",
        "train_one_step(mlp=ScaledMLP, optimizer=SimpleAdam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHHRVpQMiEvS"
      },
      "source": [
        "## e. Hyperparameter Transfer\n",
        "\n",
        "Run the following code, which will perform a sweep over learning rates for 3-layer MLPs of increasing width using Adam. How does the optimal learning rate change as the network increases in size?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "In the second cell, we will instead use the muP optimizer you implemented. How does the optimal learning rate work now? You should aim to show that there is a single global learning rate that works on a majority of widths. The 256-width network should achieve a loss of 0.5, comparable to Adam.\n",
        "\n",
        "*Answer:*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_z3UhD8Ytts"
      },
      "outputs": [],
      "source": [
        "valid_idx = np.random.randint(0, len(train_images), size=64)\n",
        "valid_images = train_images[valid_idx]\n",
        "valid_labels = train_labels[valid_idx]\n",
        "valid_images, valid_labels = valid_images.to(device), valid_labels.to(device)\n",
        "\n",
        "\n",
        "def train_with_lr(hiddens=[64, 64, 64], optimizer=SimpleAdam, lr=0.01):\n",
        "    torch.manual_seed(4)\n",
        "    np.random.seed(4)\n",
        "    model = MLP(hidden_sizes=hiddens).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optimizer(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "\n",
        "    for i in range(100):\n",
        "        batch_idx = np.random.randint(0, len(train_images), size=64)\n",
        "\n",
        "        images_batch = train_images[batch_idx]\n",
        "        labels_batch = train_labels[batch_idx]\n",
        "        images_batch, labels_batch = images_batch.to(device), labels_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(images_batch)\n",
        "        loss = criterion(outputs, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs_valid, _ = model(valid_images)\n",
        "            valid_losses = criterion(outputs_valid, valid_labels)\n",
        "            losses.append(valid_losses.item())\n",
        "\n",
        "    return np.mean(np.array(losses)[-5:])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEIPwU8SZtIz"
      },
      "outputs": [],
      "source": [
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0]\n",
        "adam_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        loss = train_with_lr(hiddens=[width, width, width], lr=lr)\n",
        "        adam_results[wi, lri] = loss\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate (Adam)')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), adam_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTHetldFdMOP"
      },
      "outputs": [],
      "source": [
        "all_widths = [4, 8, 16, 32, 64, 128, 256]\n",
        "all_lrs = [0.03, 0.1, 0.3, 1.0, 3.0, 10.0]\n",
        "mup_results = np.zeros((len(all_widths), len(all_lrs)))\n",
        "for wi, width in enumerate(all_widths):\n",
        "    for lri, lr in enumerate(all_lrs):\n",
        "        loss = train_with_lr(hiddens=[width, width, width], lr=lr, optimizer=SimpleAdamMuP)\n",
        "        mup_results[wi, lri] = loss\n",
        "\n",
        "fig, axs = plt.subplots(1, figsize=(8, 4))\n",
        "axs.set_title(f'Loss per learning rate (muP)')\n",
        "for wi, width in enumerate(all_widths):\n",
        "    axs.plot(np.arange(len(all_lrs)), mup_results[wi], label=f'Width: {width}')\n",
        "axs.set_xticks(np.arange(len(all_lrs)))\n",
        "axs.set_xticklabels(all_lrs)\n",
        "axs.set_ylim(bottom=0, top=3)\n",
        "axs.legend()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaqzanKomMTx"
      },
      "source": [
        "## e. Shampoo and Orthogonalization\n",
        "\n",
        "In lecture, we discussed a simplified version of the Shampoo update, which can be viewed as *orthogonalizing* the update to a dense layer. In the following code block, implement this simplified Shampoo update:\n",
        "\n",
        "$$\n",
        "momentum \\rightarrow U \\Sigma V^T. \\qquad update = UV^T.\n",
        "$$\n",
        "\n",
        "Feel free to use linear algebra functions such as `torch.linalg.svd`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUgzUqQ9nALJ"
      },
      "outputs": [],
      "source": [
        "from torch.optim.optimizer import Optimizer\n",
        "from typing import Any\n",
        "class SimpleShampoo(Optimizer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Any,\n",
        "        lr: float = 1e-1,\n",
        "        b1: float = 0.9,\n",
        "    ):\n",
        "        defaults = dict(lr=lr, b1=b1)\n",
        "        super(SimpleShampoo, self).__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                grad = p.grad.data\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0: # Initialization\n",
        "                    state[\"step\"] = torch.tensor(0.0)\n",
        "                    state['momentum'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "                m = state['momentum']\n",
        "                m.lerp_(grad, 1-group[\"b1\"])\n",
        "\n",
        "                ############ TODO\n",
        "                if len(m.shape) == 1:\n",
        "                    u = m # Ignore biases for this question, it's not important.\n",
        "                else:\n",
        "                    pass\n",
        "                #############\n",
        "                p.add_(u, alpha=-group['lr'])\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIoTUgUYmvJt"
      },
      "source": [
        "Now, we will examine the relationship between the Frobenius norm and the Spectral norm for Adam vs. Shampoo. Plot these norms using your code from part c. What relationship do you see? Can you come up for a reason why this makes sense?\n",
        "\n",
        "*Answer:*\n",
        "\n",
        "Bonus: How should we scale the Shampoo update so the *induced RMS-RMS norm* is equal? Implement this change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pcGr3WloaGw"
      },
      "outputs": [],
      "source": [
        "train_one_step_matrices(optimizer=SimpleAdam)\n",
        "train_one_step_matrices(optimizer=SimpleShampoo, label=\"Shampoo\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
